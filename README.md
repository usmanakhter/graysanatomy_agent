# ğŸ§  Gray's Anatomy AI Assistant

Intelligent anatomy question-answering system using RAG (Retrieval-Augmented Generation) architecture.

## ğŸ¯ Current Status: **Slice 1 Complete** âœ…

### What's Working:
- âœ… **Question Answering**: Ask any anatomy question
- âœ… **BM25 Search**: Fast keyword-based retrieval
- âœ… **Mistral-7B LLM**: Free HuggingFace API
- âœ… **Source Attribution**: See which text chunks were used
- âœ… **Performance Metrics**: Track response times
- âœ… **100% Free**: No paid APIs

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/graysanatomy_agent.git
cd graysanatomy_agent
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Set API Token
Get a free token at https://huggingface.co/settings/tokens

**Local Development:**
```bash
```

**Streamlit Cloud:**
Add to Settings â†’ Secrets:
```toml
```

### 4. Run Application
```bash
streamlit run app.py
```

### 5. First Run
- Downloads Gray's Anatomy text (~30 seconds)
- Builds BM25 search index (~2 minutes)
- Subsequent runs are instant!

## ğŸ“ Project Structure

```
graysanatomy_agent/
â”œâ”€â”€ app.py                 # Streamlit UI (main entry point)
â”œâ”€â”€ orchestrator.py        # Central routing logic
â”œâ”€â”€ config.py             # All configuration options
â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ search.py         # Search strategies (BM25, FAISS, hybrid)
â”‚   â”œâ”€â”€ llm_hub.py        # LLM provider management
â”‚   â””â”€â”€ graph_rag.py      # Knowledge graph (Slice 4)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ loader.py         # Text loading and chunking
â”‚
â””â”€â”€ indexes/              # Generated indexes (auto-created)
    â”œâ”€â”€ bm25_index.pkl
    â”œâ”€â”€ vector_store/
    â””â”€â”€ knowledge_graph.pkl
```

## ğŸ—ºï¸ Roadmap

### âœ… Slice 1: Core System (CURRENT)
- [x] Basic RAG pipeline
- [x] BM25 lexical search
- [x] Single LLM (Mistral-7B)
- [x] Streamlit UI
- [x] Source attribution

### ğŸ”„ Slice 2: Multiple LLMs (NEXT)
- [ ] Add 5 more LLM options
- [ ] Mistral-7B, Mixtral-8x7B
- [ ] Zephyr-7B, Llama-3.1-8B
- [ ] Phi-3-Mini, BioMedLM
- [ ] Dropdown selector in UI

### ğŸ“‹ Slice 3: Advanced Search
- [ ] Semantic search (FAISS + embeddings)
- [ ] 5 embedding model options
- [ ] Hybrid search (BM25 + Vector + RRF)
- [ ] Search strategy selector

### ğŸ•¸ï¸ Slice 4: Knowledge Graph
- [ ] Entity extraction (spaCy)
- [ ] Relationship detection
- [ ] Simple graph (NetworkX)
- [ ] Full GraphRAG (community detection)
- [ ] Graph visualization

## ğŸ¨ Features by Slice

| Feature | Slice 1 | Slice 2 | Slice 3 | Slice 4 |
|---------|---------|---------|---------|---------|
| **Question Answering** | âœ… | âœ… | âœ… | âœ… |
| **Lexical Search** | âœ… | âœ… | âœ… | âœ… |
| **Semantic Search** | âŒ | âŒ | âœ… | âœ… |
| **Hybrid Search** | âŒ | âŒ | âœ… | âœ… |
| **Single LLM** | âœ… | âŒ | âŒ | âŒ |
| **Multiple LLMs** | âŒ | âœ… | âœ… | âœ… |
| **Knowledge Graph** | âŒ | âŒ | âŒ | âœ… |
| **Graph Visualization** | âŒ | âŒ | âŒ | âœ… |

## ğŸ’¡ Example Usage

```python
# Ask a question
"What are the main bones of the skull?"

# System automatically:
1. Searches Gray's Anatomy text (BM25)
2. Retrieves top 5 relevant chunks
3. Sends to Mistral-7B with context
4. Returns comprehensive answer
5. Shows sources used
```

## ğŸ”§ Configuration

Edit `config.py` to customize:

```python
# Chunking
CHUNK_SIZE = 1000          # Characters per chunk
CHUNK_OVERLAP = 200        # Overlap between chunks

# Retrieval
TOP_K_RESULTS = 5          # Number of chunks to retrieve

# LLM (Slice 2+)
DEFAULT_LLM = "mistral-7b"

# Search (Slice 3+)
DEFAULT_SEARCH_STRATEGY = "hybrid"
```

## ğŸ“Š Performance

**Slice 1 Metrics:**
- First query: ~5-7 seconds (including model warmup)
- Subsequent queries: ~2-3 seconds
- Index build time: ~2 minutes (one-time)
- Memory usage: ~300 MB

## ğŸŒ Deployment

### Streamlit Cloud (Free)
1. Push to GitHub
2. Connect at share.streamlit.io
3. Add `HUGGINGFACEHUB_API_TOKEN` in Secrets
4. Deploy!

**Limitations:**
- 1GB RAM (fine for Slice 1-2)
- May struggle with Slice 3-4
- Consider HuggingFace Spaces for advanced features

### HuggingFace Spaces (Better for ML)
1. Create Space at huggingface.co/spaces
2. Upload code
3. Add `HUGGINGFACEHUB_API_TOKEN` secret
4. Automatic deployment

**Benefits:**
- 16GB RAM
- Better for embeddings & graphs
- Free tier available

## ğŸ¤ Contributing

This is a vertical slice architecture:
1. Each slice is fully functional
2. New features add horizontal choices
3. Easy to understand and extend

To add a new feature:
1. Determine which slice it belongs to
2. Update relevant component
3. Add option to `config.py`
4. Update UI dropdown in `app.py`

## ğŸ“ License

MIT License - Feel free to use and modify!

## ğŸ™ Acknowledgments

- **Gray's Anatomy (1918)** - Public domain medical text
- **HuggingFace** - Free LLM inference API
- **Streamlit** - Excellent Python web framework

## ğŸ“§ Support

Questions? Issues? 
- Open a GitHub issue
- Check the [ARCHITECTURE.md](ARCHITECTURE.md) for design details

---

**Built with â¤ï¸ using Vertical Slice Architecture**