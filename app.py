"""
Streamlit UI for Gray's Anatomy AI Agent
"""
import streamlit as st
import os

# Import our modules
from search import search, load_search_index
from llm import get_answer
from config import USE_OPENAI, LLM_MODEL, TOP_K_RESULTS

# ---------------------
# Page Config
# ---------------------
st.set_page_config(
    page_title="Gray's Anatomy AI",
    layout="wide",
    page_icon="üß†"
)

# ---------------------
# Header
# ---------------------
st.title("üß† Gray's Anatomy AI Assistant")
st.markdown("""
Ask questions about human anatomy based on **Gray's Anatomy (1918 edition)**.
Uses BM25 keyword search + LLM for intelligent answers.
""")

# ---------------------
# Sidebar
# ---------------------
with st.sidebar:
    st.header("‚öôÔ∏è System Info")
    
    model_name = "OpenAI GPT-3.5" if USE_OPENAI else LLM_MODEL.split('/')[-1]
    st.markdown(f"""
    **Current Setup:**
    - ü§ñ LLM: {model_name}
    - üîç Search: BM25 (keyword)
    - üìö Source: Gray's Anatomy (1918)
    - üéØ Results: Top {TOP_K_RESULTS} chunks
    """)
    
    st.markdown("---")
    st.subheader("üìä Status")
    if os.path.exists("bm25_index.pkl"):
        st.success("‚úÖ Search index ready")
    else:
        st.warning("‚è≥ Building index on first query...")

    # API Keys check
    if USE_OPENAI:
        if os.environ.get("OPENAI_API_KEY"):
            st.success("‚úÖ OpenAI key configured")
        else:
            st.error("‚ùå OPENAI_API_KEY missing")
    else:
        if os.environ.get("HUGGINGFACEHUB_API_TOKEN"):
            st.success("‚úÖ HuggingFace token configured")
        else:
            st.error("‚ùå HUGGINGFACEHUB_API_TOKEN missing")

    st.markdown("---")
    st.subheader("üí° Example Questions")

    # List of sample questions
    examples = [
        "What are the bones of the skull?",
        "Describe the heart chambers",
        "What is the brachial plexus?",
        "Explain the vertebral column",
        "What muscles control breathing?",
        "Describe the liver structure",
        "What are the cranial nerves?",
        "Explain the knee joint",
        "What is the spinal cord?",
        "Describe blood circulation"
    ]

    # Add buttons for each example
    for i, ex in enumerate(examples):
        if st.button(ex, key=f"example_{i}"):
            st.session_state["selected_example"] = ex

# ---------------------
# Initialize search index (cached)
# ---------------------
@st.cache_resource
def init_system():
    try:
        bm25, chunks = load_search_index()
        return True, f"Ready! Searching {len(chunks):,} text chunks"
    except Exception as e:
        return False, f"Error: {str(e)}"

# Initialize system
with st.spinner("üîß Initializing system..."):
    success, message = init_system()

if not success:
    st.error(f"‚ùå {message}")
    st.stop()
else:
    st.success(f"‚úÖ {message}")

# ---------------------
# Initialize chat history
# ---------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ---------------------
# User Input
# ---------------------
col1, col2 = st.columns([5, 1])

with col1:
    # Populate text input with example if clicked
    default_question = st.session_state.pop("selected_example", "") if "selected_example" in st.session_state else ""
    user_question = st.text_input(
        "Your question:",
        value=default_question,
        placeholder="e.g., What are the bones of the skull?",
        key="question_input"
    )

with col2:
    ask_button = st.button("üîç Ask", type="primary", use_container_width=True)

# Clear chat button
if st.button("üóëÔ∏è Clear Chat"):
    st.session_state.chat_history = []

# ---------------------
# Process question
# ---------------------
if (ask_button or default_question) and user_question.strip():
    # Avoid duplicate answers on rerun
    last_question = st.session_state.chat_history[-1]["question"] if st.session_state.chat_history else None
    if user_question != last_question:
        with st.spinner("üîç Searching... ü§î Thinking..."):
            try:
                # Search top-k relevant chunks
                relevant_chunks = search(user_question, k=TOP_K_RESULTS)
                context = "\n\n".join(relevant_chunks)

                # Get LLM answer
                answer = get_answer(context, user_question)

                # Append to chat history
                st.session_state.chat_history.append({
                    "question": user_question,
                    "answer": answer
                })

            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                if "HUGGINGFACEHUB_API_TOKEN" in str(e):
                    st.info("üí° Add your HuggingFace token in Streamlit Cloud: Settings ‚Üí Secrets")
                elif "OPENAI_API_KEY" in str(e):
                    st.info("üí° Add your OpenAI key in Streamlit Cloud: Settings ‚Üí Secrets")

# ---------------------
# Display chat history
# ---------------------
st.markdown("---")
if st.session_state.chat_history:
    st.subheader("üí¨ Conversation History")
    for chat in reversed(st.session_state.chat_history):
        st.markdown(f"### ‚ùì {chat['question']}")
        st.markdown(chat['answer'])
        st.markdown("---")
else:
    st.info("üëÜ Ask a question above or click an example from the sidebar to get started!")

# ---------------------
# Footer
# ---------------------
st.caption("""
‚ö†Ô∏è **Disclaimer:** This AI uses Gray's Anatomy (1918 edition). While anatomical fundamentals 
remain accurate, medical terminology and some concepts may reflect the knowledge of that era.
Always consult current medical resources for clinical decisions.
""")

# ---------------------
# Debug info
# ---------------------
with st.expander("üîß Debug Info"):
    st.json({
        "USE_OPENAI": USE_OPENAI,
        "LLM_MODEL": LLM_MODEL,
        "TOP_K_RESULTS": TOP_K_RESULTS,
        "Index exists": os.path.exists("bm25_index.pkl"),
        "Text file exists": os.path.exists("grays_anatomy.txt"),
        "Chat history length": len(st.session_state.chat_history)
    })
